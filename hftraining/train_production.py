#!/usr/bin/env python3
"""
Production-Ready Training Script with All Fixes
Addresses all identified issues and scales up for production
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
import numpy as np
import pandas as pd
from pathlib import Path
import json
import time
from datetime import datetime
import logging
from typing import Dict, List, Optional, Tuple
import warnings
import os
import sys

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from hftraining.robust_data_pipeline import (
    create_robust_dataloader, 
    download_and_process_stocks,
    AdvancedDataProcessor
)
from hftraining.modern_optimizers import get_optimizer
from hftraining.improved_schedulers import get_improved_scheduler
from hftraining.logging_utils import get_logger, MetricsTracker

warnings.filterwarnings('ignore')

class ScaledTransformerModel(nn.Module):
    """Enhanced transformer model with proper scaling"""
    
    def __init__(self, config: Dict):
        super().__init__()
        
        # Ensure proper dimensions
        hidden_size = config['hidden_size']
        num_heads = config['num_heads']
        
        # Fix: Ensure hidden_size is divisible by num_heads
        if hidden_size % num_heads != 0:
            # Adjust hidden_size to be divisible
            hidden_size = (hidden_size // num_heads) * num_heads
            logging.warning(f"Adjusted hidden_size to {hidden_size} for compatibility")
        
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.num_layers = config['num_layers']
        
        # Input projection with flexible input size
        self.input_projection = nn.Linear(config['input_features'], hidden_size)
        
        # Positional encoding
        self.positional_encoding = self._create_positional_encoding(
            config['sequence_length'], hidden_size
        )
        
        # Transformer layers with gradient checkpointing support
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=config.get('intermediate_size', hidden_size * 4),
            dropout=config.get('dropout', 0.1),
            activation='gelu',
            batch_first=True,
            norm_first=True  # Pre-LN for better stability
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_layers,
            enable_nested_tensor=False  # Disable for compatibility
        )
        
        # Output heads
        self.price_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Dropout(config.get('dropout', 0.1)),
            nn.Linear(hidden_size // 2, config['prediction_horizon'] * config['input_features'])
        )
        
        self.action_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Dropout(config.get('dropout', 0.1)),
            nn.Linear(hidden_size // 2, 3)  # Buy, Hold, Sell
        )
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        # Initialize weights
        self.apply(self._init_weights)
    
    def _create_positional_encoding(self, seq_len: int, hidden_size: int) -> torch.Tensor:
        """Create sinusoidal positional encoding"""
        pe = torch.zeros(seq_len, hidden_size)
        position = torch.arange(0, seq_len).unsqueeze(1).float()
        
        div_term = torch.exp(
            torch.arange(0, hidden_size, 2).float() * 
            -(np.log(10000.0) / hidden_size)
        )
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)
    
    def _init_weights(self, module):
        """Initialize weights with Xavier/Kaiming"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)
    
    def forward(self, input_ids, attention_mask=None, use_checkpointing=False):
        """Forward pass with optional gradient checkpointing"""
        batch_size, seq_len, features = input_ids.shape
        
        # Project input
        hidden_states = self.input_projection(input_ids)
        
        # Add positional encoding
        hidden_states = hidden_states + self.positional_encoding[:, :seq_len, :].to(input_ids.device)
        
        # Apply layer norm
        hidden_states = self.layer_norm(hidden_states)
        
        # Don't use attention mask for now - it's causing issues
        # The transformer will process all positions equally
        attention_mask = None
        
        # Transformer encoding
        if use_checkpointing and self.training:
            # Use gradient checkpointing for memory efficiency
            hidden_states = torch.utils.checkpoint.checkpoint(
                self.transformer, hidden_states
            )
        else:
            hidden_states = self.transformer(hidden_states)
        
        # Pool over sequence (use mean pooling)
        hidden_states = hidden_states.mean(dim=1)
        
        # Generate outputs
        price_predictions = self.price_head(hidden_states)
        action_logits = self.action_head(hidden_states)
        
        return {
            'price_predictions': price_predictions,
            'action_logits': action_logits,
            'hidden_states': hidden_states
        }


class ProductionTrainer:
    """Production-ready trainer with all fixes and enhancements"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Setup logging
        self.logger = get_logger(
            log_dir=config.get('log_dir', 'hftraining/logs'),
            experiment_name="production_training"
        )
        
        self.metrics_tracker = MetricsTracker()
        
        # Initialize model
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        
        # Training state
        self.global_step = 0
        self.epoch = 0
        self.best_loss = float('inf')
        self.patience_counter = 0
        
        # Checkpoint directory
        self.checkpoint_dir = Path(config.get('checkpoint_dir', 'hftraining/checkpoints'))
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    def setup_model(self, input_features: int):
        """Setup model with proper configuration"""
        model_config = {
            'hidden_size': self.config.get('hidden_size', 512),
            'num_heads': self.config.get('num_heads', 16),
            'num_layers': self.config.get('num_layers', 8),
            'intermediate_size': self.config.get('intermediate_size', 2048),
            'dropout': self.config.get('dropout', 0.15),
            'sequence_length': self.config.get('sequence_length', 60),
            'prediction_horizon': self.config.get('prediction_horizon', 5),
            'input_features': input_features
        }
        
        # Ensure compatibility
        if model_config['hidden_size'] % model_config['num_heads'] != 0:
            model_config['hidden_size'] = (model_config['hidden_size'] // model_config['num_heads']) * model_config['num_heads']
            self.logger.warning(f"Adjusted hidden_size to {model_config['hidden_size']}")
        
        self.model = ScaledTransformerModel(model_config)
        self.model.to(self.device)
        
        # Setup multi-GPU if available
        if torch.cuda.device_count() > 1 and self.config.get('use_multi_gpu', True):
            self.logger.info(f"Using {torch.cuda.device_count()} GPUs")
            self.model = nn.DataParallel(self.model)
        
        # Count parameters
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        self.logger.info(f"Model parameters: {total_params:,} total, {trainable_params:,} trainable")
        
        return model_config
    
    def setup_optimizer(self):
        """Setup optimizer with proper configuration"""
        optimizer_name = self.config.get('optimizer', 'adamw')
        lr = self.config.get('learning_rate', 1e-4)
        weight_decay = self.config.get('weight_decay', 0.01)
        
        # Get optimizer
        optimizer_kwargs = {
            'lr': lr,
            'weight_decay': weight_decay,
            'betas': (
                self.config.get('adam_beta1', 0.9),
                self.config.get('adam_beta2', 0.999),
            ),
            'eps': self.config.get('adam_epsilon', 1e-8),
        }
        if optimizer_name.lower() == 'muon':
            optimizer_kwargs.update({
                'momentum': self.config.get('muon_momentum', 0.95),
                'nesterov': self.config.get('muon_nesterov', True),
                'ns_steps': self.config.get('muon_ns_steps', 5),
                'adamw_lr': self.config.get('muon_adamw_lr'),
            })

        self.optimizer = get_optimizer(
            optimizer_name,
            self.model.parameters(),
            **optimizer_kwargs,
        )
        
        # Setup scheduler
        scheduler_name = self.config.get('scheduler', 'cosine_restart')
        scheduler_kwargs = self.config.get('scheduler_kwargs', {}) or {}

        try:
            if scheduler_name in {"cosine_restart", "linear_warmup_cosine", "cyclical", "polynomial", "muon", "warmup_hold_cosine"}:
                # Use improved scheduler factory (supports 'muon')
                self.scheduler = get_improved_scheduler(self.optimizer, scheduler_name, **scheduler_kwargs)
            else:
                # Fallback to previous default
                self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                    self.optimizer,
                    T_0=500,
                    T_mult=2,
                    eta_min=1e-6,
                )
        except Exception as e:
            self.logger.warning(f"Failed to create scheduler '{scheduler_name}': {e}; falling back to CosineAnnealingWarmRestarts")
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=500,
                T_mult=2,
                eta_min=1e-6,
            )
        
        # Setup mixed precision
        if self.config.get('use_mixed_precision', True) and self.device.type == 'cuda':
            self.scaler = GradScaler()
            self.logger.info("Mixed precision training enabled")
        
        self.logger.info(f"Optimizer: {optimizer_name}, LR: {lr}, Weight Decay: {weight_decay}")
    
    def compute_loss(self, outputs: Dict, batch: Dict) -> Tuple[torch.Tensor, Dict]:
        """Compute loss with proper error handling"""
        losses = {}
        
        # Price prediction loss
        price_predictions = outputs['price_predictions']
        batch_size = price_predictions.size(0)
        
        # Reshape predictions to match targets
        pred_horizon = self.config.get('prediction_horizon', 5)
        num_features = batch['labels'].size(-1)
        price_predictions = price_predictions.view(batch_size, pred_horizon, num_features)
        
        # MSE loss for price prediction
        price_loss = F.mse_loss(price_predictions, batch['labels'])
        losses['price_loss'] = price_loss
        
        # Action classification loss
        if 'action_labels' in batch and batch['action_labels'].numel() > 0:
            action_logits = outputs['action_logits']
            action_labels = batch['action_labels'].squeeze(-1)
            
            # Ensure dimensions match
            if action_logits.size(0) == action_labels.size(0):
                action_loss = F.cross_entropy(action_logits, action_labels)
                losses['action_loss'] = action_loss
            else:
                self.logger.warning(f"Batch size mismatch: {action_logits.size(0)} vs {action_labels.size(0)}")
                losses['action_loss'] = torch.tensor(0.0).to(self.device)
        
        # Combined loss
        total_loss = losses['price_loss'] + self.config.get('action_loss_weight', 0.5) * losses.get('action_loss', 0)
        losses['total_loss'] = total_loss
        
        return total_loss, losses
    
    def train_step(self, batch: Dict) -> Dict:
        """Single training step with gradient accumulation"""
        self.model.train()
        
        # Move batch to device
        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                 for k, v in batch.items()}
        
        # Mixed precision context
        if self.scaler is not None:
            with autocast():
                outputs = self.model(
                    batch['input_ids'], 
                    batch.get('attention_mask'),
                    use_checkpointing=self.config.get('gradient_checkpointing', True)
                )
                loss, loss_dict = self.compute_loss(outputs, batch)
                
                # Scale loss for gradient accumulation
                loss = loss / self.config.get('gradient_accumulation_steps', 1)
            
            # Backward pass with scaling
            self.scaler.scale(loss).backward()
        else:
            outputs = self.model(
                batch['input_ids'], 
                batch.get('attention_mask'),
                use_checkpointing=self.config.get('gradient_checkpointing', True)
            )
            loss, loss_dict = self.compute_loss(outputs, batch)
            loss = loss / self.config.get('gradient_accumulation_steps', 1)
            loss.backward()
        
        return {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in loss_dict.items()}
    
    def train_epoch(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None):
        """Train for one epoch with proper error handling"""
        epoch_losses = []
        accumulation_steps = self.config.get('gradient_accumulation_steps', 1)
        
        for batch_idx, batch in enumerate(train_loader):
            try:
                # Skip invalid batches
                if batch['input_ids'].size(0) == 0:
                    self.logger.warning(f"Skipping empty batch {batch_idx}")
                    continue
                
                # Training step
                loss_dict = self.train_step(batch)
                epoch_losses.append(loss_dict)
                
                # Gradient accumulation
                if (batch_idx + 1) % accumulation_steps == 0:
                    # Gradient clipping
                    if self.scaler is not None:
                        self.scaler.unscale_(self.optimizer)
                    
                    max_grad_norm = self.config.get('max_grad_norm', 1.0)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
                    
                    # Optimizer step
                    if self.scaler is not None:
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        self.optimizer.step()
                    
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    
                    self.global_step += 1
                    
                    # Logging
                    if self.global_step % self.config.get('log_interval', 50) == 0:
                        avg_loss = np.mean([l['total_loss'] for l in epoch_losses[-accumulation_steps:]])
                        lr = self.scheduler.get_last_lr()[0]
                        
                        self.logger.log_step_metrics(
                            self.global_step,
                            {'loss': avg_loss, 'lr': lr},
                            phase='train'
                        )
                        
                        self.metrics_tracker.add_metric(self.global_step, 'train', loss=avg_loss, lr=lr)
                    
                    # Validation
                    if val_loader and self.global_step % self.config.get('eval_interval', 500) == 0:
                        val_loss = self.validate(val_loader)
                        
                        # Early stopping
                        if val_loss < self.best_loss:
                            self.best_loss = val_loss
                            self.patience_counter = 0
                            self.save_checkpoint('best')
                        else:
                            self.patience_counter += 1
                            
                            if self.patience_counter >= self.config.get('patience', 10):
                                self.logger.log_early_stopping(self.global_step, self.patience_counter)
                                return False
                    
                    # Regular checkpoint
                    if self.global_step % self.config.get('checkpoint_interval', 1000) == 0:
                        self.save_checkpoint(f'step_{self.global_step}')
                    
                    # Check max steps
                    if self.global_step >= self.config.get('max_steps', 10000):
                        return False
                        
            except Exception as e:
                self.logger.error(f"Error in batch {batch_idx}: {e}")
                continue
        
        return True
    
    def validate(self, val_loader: DataLoader) -> float:
        """Validation with error handling"""
        self.model.eval()
        val_losses = []
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    # Skip invalid batches
                    if batch['input_ids'].size(0) == 0:
                        continue
                    
                    # Move to device
                    batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                            for k, v in batch.items()}
                    
                    # Forward pass
                    outputs = self.model(batch['input_ids'], batch.get('attention_mask'))
                    loss, loss_dict = self.compute_loss(outputs, batch)
                    
                    val_losses.append(loss_dict['total_loss'])
                    
                except Exception as e:
                    self.logger.warning(f"Validation batch error: {e}")
                    continue
        
        if val_losses:
            avg_val_loss = np.mean(val_losses)
            self.logger.log_step_metrics(self.global_step, {'val_loss': avg_val_loss}, phase='validation')
            self.metrics_tracker.add_metric(self.global_step, 'validation', loss=avg_val_loss)
            return avg_val_loss
        
        return float('inf')
    
    def save_checkpoint(self, name: str):
        """Save training checkpoint"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'epoch': self.epoch,
            'best_loss': self.best_loss,
            'config': self.config
        }
        
        if self.scaler is not None:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        path = self.checkpoint_dir / f'{name}.pt'
        torch.save(checkpoint, path)
        self.logger.log_checkpoint_saved(self.global_step, str(path))
    
    def load_checkpoint(self, path: str):
        """Load training checkpoint"""
        checkpoint = torch.load(path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if self.scaler is not None and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        self.global_step = checkpoint['global_step']
        self.epoch = checkpoint['epoch']
        self.best_loss = checkpoint['best_loss']
        
        self.logger.info(f"Checkpoint loaded from step {self.global_step}")
    
    def train(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None):
        """Main training loop"""
        model_info = {'parameters': sum(p.numel() for p in self.model.parameters())}
        self.logger.log_training_start(self.config, model_info)
        
        max_epochs = self.config.get('max_epochs', 100)
        
        try:
            for epoch in range(max_epochs):
                self.epoch = epoch
                self.logger.log_epoch_start(epoch+1, max_epochs)
                
                # Train epoch
                should_continue = self.train_epoch(train_loader, val_loader)
                
                if not should_continue:
                    break
                
                # Save epoch checkpoint
                self.save_checkpoint(f'epoch_{epoch+1}')
            
            # Final checkpoint
            self.save_checkpoint('final')
            
        except KeyboardInterrupt:
            self.logger.info("Training interrupted by user")
            self.save_checkpoint('interrupted')
        
        except Exception as e:
            self.logger.log_error(e, self.global_step)
            self.save_checkpoint('error')
            raise
        
        finally:
            # Save metrics
            self.metrics_tracker.save(self.checkpoint_dir / 'metrics.json')
            final_metrics = {'final_step': self.global_step, 'best_loss': self.best_loss}
            self.logger.log_training_complete(0, final_metrics)


def main():
    """Main training function"""
    
    # Production configuration
    config = {
        # Model
        'hidden_size': 512,
        'num_heads': 16,
        'num_layers': 8,
        'intermediate_size': 2048,
        'dropout': 0.15,
        
        # Data
        'sequence_length': 60,
        'prediction_horizon': 5,
        'batch_size': 32,
        'num_workers': 4,
        
        # Training
        'optimizer': 'adamw',
        'learning_rate': 5e-5,
        'weight_decay': 0.01,
        'max_steps': 10000,
        'max_epochs': 100,
        'warmup_steps': 500,
        'gradient_accumulation_steps': 4,
        'max_grad_norm': 1.0,
        
        # Features
        'use_mixed_precision': True,
        'use_multi_gpu': True,
        'gradient_checkpointing': True,
        
        # Logging
        'log_interval': 50,
        'eval_interval': 500,
        'checkpoint_interval': 1000,
        
        # Early stopping
        'patience': 10,
        'action_loss_weight': 0.5,
        
        # Directories
        'log_dir': 'hftraining/logs/production',
        'checkpoint_dir': 'hftraining/checkpoints/production'
    }
    
    # Download and process data
    print("Downloading and processing stock data...")
    symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'META', 'NVDA']
    
    try:
        data, feature_names = download_and_process_stocks(
            symbols, 
            start_date='2018-01-01'
        )
        print(f"Data shape: {data.shape}")
        print(f"Features: {feature_names[:10]}...")
        
    except Exception as e:
        print(f"Failed to download data: {e}")
        print("Using synthetic data for testing...")
        np.random.seed(42)
        data = np.random.randn(10000, 20)
        feature_names = [f'feature_{i}' for i in range(20)]
    
    # Split data
    train_size = int(0.8 * len(data))
    val_size = int(0.1 * len(data))
    
    train_data = data[:train_size]
    val_data = data[train_size:train_size+val_size]
    test_data = data[train_size+val_size:]
    
    print(f"Train: {train_data.shape}, Val: {val_data.shape}, Test: {test_data.shape}")
    
    # Create dataloaders
    train_loader = create_robust_dataloader(
        train_data,
        batch_size=config['batch_size'],
        sequence_length=config['sequence_length'],
        prediction_horizon=config['prediction_horizon'],
        shuffle=True,
        num_workers=config['num_workers'],
        augment=True
    )
    
    val_loader = create_robust_dataloader(
        val_data,
        batch_size=config['batch_size'],
        sequence_length=config['sequence_length'],
        prediction_horizon=config['prediction_horizon'],
        shuffle=False,
        num_workers=2,
        augment=False
    )
    
    # Initialize trainer
    trainer = ProductionTrainer(config)
    
    # Setup model
    input_features = data.shape[1]
    trainer.setup_model(input_features)
    
    # Setup optimizer
    trainer.setup_optimizer()
    
    # Check for resume
    checkpoint_path = Path(config['checkpoint_dir']) / 'interrupted.pt'
    if checkpoint_path.exists():
        print(f"Resuming from checkpoint: {checkpoint_path}")
        trainer.load_checkpoint(str(checkpoint_path))
    
    # Start training
    trainer.train(train_loader, val_loader)
    
    print("Training completed successfully!")
    
    # Generate report
    report_path = Path('hftraining/reports') / f'production_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_path, 'w') as f:
        f.write(f"# Production Training Report\n")
        f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"## Configuration\n")
        f.write(f"```json\n{json.dumps(config, indent=2)}\n```\n\n")
        f.write(f"## Results\n")
        f.write(f"- Final Step: {trainer.global_step}\n")
        f.write(f"- Best Validation Loss: {trainer.best_loss:.4f}\n")
        f.write(f"- Training Completed: Yes\n\n")
        f.write(f"## Next Steps\n")
        f.write(f"1. Evaluate on test set\n")
        f.write(f"2. Deploy model for inference\n")
        f.write(f"3. Monitor performance in production\n")
    
    print(f"Report saved: {report_path}")


if __name__ == "__main__":
    main()
