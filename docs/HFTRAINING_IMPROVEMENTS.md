# HFTraining Architecture Improvements

## Critical Issues Found

### 1. Massive Code Duplication
- **9 separate training scripts** (train_*.py) with overlapping functionality
- **12 different Trainer classes** doing similar work
- **5 TransformerModel variants** with minimal differences
- **6 data loading functions** with redundant code

### 2. Configuration Chaos
- Config module exists but only 1/9 training files uses it
- Hardcoded hyperparameters scattered across files
- No centralized experiment tracking

### 3. Unused Advanced Features
- Modern optimizers (Shampoo, MUON) implemented but unused
- All trainers defaulting to AdamW
- No distributed training integration despite having the code

## Top Priority Improvements

### 1. Unified Training Framework
```python
# hftraining/core/base_trainer.py
class UnifiedTrainer:
    """Single trainer to rule them all"""
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.model = ModelFactory.create(config.model)
        self.optimizer = OptimizerFactory.create(config.optimizer)
        self.data_loader = DataLoaderFactory.create(config.data)
```

### 2. Model Registry Pattern
```python
# hftraining/models/registry.py
MODEL_REGISTRY = {
    'transformer': TransformerModel,
    'dit': DiTModel,
    'lstm': LSTMModel,
}

def get_model(name: str, **kwargs):
    return MODEL_REGISTRY[name](**kwargs)
```

### 3. Centralized Data Pipeline
```python
# hftraining/data/pipeline.py
class UnifiedDataPipeline:
    """Single data loading interface"""
    def __init__(self, config: DataConfig):
        self.loaders = {
            'csv': CSVLoader(),
            'parquet': ParquetLoader(),
            'api': APILoader(),
        }
    
    def load(self) -> Dataset:
        # Auto-detect and load from trainingdata/
        pass
```

### 4. Config-Driven Everything
```yaml
# configs/experiment.yaml
model:
  type: transformer
  hidden_size: 512
  num_layers: 8

optimizer:
  type: shampoo  # Use modern optimizers!
  lr: 3e-4
  
data:
  source: local
  symbols: [AAPL, GOOGL]
  
training:
  epochs: 100
  mixed_precision: true
  distributed: true
```

### 5. Experiment Management
```python
# hftraining/experiment.py
class ExperimentManager:
    def run(self, config_path: str):
        config = load_config(config_path)
        trainer = UnifiedTrainer(config)
        results = trainer.train()
        self.log_results(results)
        self.save_artifacts()
```

## Implementation Roadmap

### Phase 1: Core Refactor (Week 1)
1. Create UnifiedTrainer base class
2. Consolidate model implementations
3. Build model/optimizer factories

### Phase 2: Data Pipeline (Week 2)
1. Merge all data loading functions
2. Create unified DataLoader class
3. Add caching and preprocessing

### Phase 3: Config System (Week 3)
1. Move all hardcoded params to configs
2. Add config validation
3. Create experiment templates

### Phase 4: Testing & Migration (Week 4)
1. Comprehensive test suite
2. Migrate existing scripts to new system
3. Performance benchmarking

## Quick Wins (Do Today)

1. **Delete duplicate code** - Merge the 9 train_*.py files
2. **Use existing config.py** - Wire it into all trainers
3. **Enable Shampoo/MUON** - These are already implemented!
4. **Add pytest fixtures** - Reduce test duplication

## Performance Optimizations

1. **Batch Processing**: Combine small operations
2. **Data Prefetching**: Use DataLoader num_workers
3. **Gradient Accumulation**: For larger effective batch sizes
4. **Compile Models**: Use torch.compile() for 2x speedup
5. **Profile First**: Use torch.profiler before optimizing

## Testing Strategy

```python
# tests/conftest.py
@pytest.fixture
def base_config():
    return TrainingConfig(...)

@pytest.fixture  
def sample_data():
    return load_test_data()

# tests/experimental/hf/test_unified_trainer.py
def test_all_optimizers(base_config, sample_data):
    for opt in ['adamw', 'shampoo', 'muon']:
        config = base_config.copy()
        config.optimizer.type = opt
        trainer = UnifiedTrainer(config)
        # Test training loop
```

## Metrics to Track

- Training time reduction: Target 50% faster
- Memory usage: Target 30% less
- Code lines: Target 60% reduction
- Test coverage: Target 90%+
- Experiment reproducibility: 100%

## Anti-Patterns to Avoid

❌ Multiple scripts doing the same thing
❌ Hardcoded hyperparameters
❌ Untested code paths
❌ Copy-paste programming
❌ Ignoring existing utilities

## Summary

The codebase has good components but terrible organization. A unified framework would:
- Reduce 9 scripts to 1
- Enable easy experimentation
- Use modern optimizers already implemented
- Improve maintainability by 10x
- Make testing comprehensive

Focus on **consolidation** over new features.
