# Deep Analysis: trade_stock_e2e.py PAPER Mode Performance

## Profile Data Summary

**Actual Captured Time**: 25 milliseconds (imports only)
**Total Execution Time**: ~150 seconds (2.5 minutes)
**Phase**: Startup through initial model loading and analysis

## What the Numbers Tell Us

### The 25ms Import Phase

The cProfile data captured just the module import overhead:

```
17,235 function calls in 0.025 seconds

Top time consumers:
- marshal.loads:        3ms  (12%) - Loading compiled Python bytecode
- __build_class__:      2ms  (8%)  - Creating class objects during import
- pathlib module:       1ms  (4%)  - Filesystem path operations
- regex compilation:    1ms  (4%)  - Pattern compilation in re module
- importlib overhead:   18ms (72%) - Import system machinery
```

**Key Insight**: Python imports are actually quite fast. The entire dependency tree loads in 25ms.

### The Missing 149.975 Seconds

From execution logs, here's what happened after imports:

#### Phase 1: Alpaca Connection (0-1s)
```
âœ“ Initialized Alpaca Trading Client: PAPER account
âœ“ Retrieved account info and positions
âœ“ Market status check
```
**Performance**: Excellent, ~1 second total

#### Phase 2: Model Loading (1-50s) - **Major Bottleneck**
```
Timeline:
00:14:18 - Started loading Toto pipeline
00:14:18 - Model selection: toto (from hyperparamstore)
00:14:18 - Loaded hyperparameters
00:14:23 - GPU memory: 609.7 MB allocated (5 seconds to load weights)
00:15:05 - First inference complete (42 seconds for torch.compile + first run)
```

**ğŸ”´ Critical Finding**: **42-47 seconds spent on torch.compile + first inference**

Breakdown:
- Model weight loading: ~5s
- torch.compile (inductor): ~35-40s (first-time compilation)
- First inference: ~2-5s

#### Phase 3: Data Fetching (50-55s)
```
âœ“ BTCUSD data download
âœ“ Spread calculation (1.0019x)
âœ“ Kronos hyperparameters loaded
```
**Performance**: Fast, ~5 seconds

#### Phase 4: MaxDiff Strategy Evaluation (55-150s) - **95 seconds**
```
Evaluated ~30 symbols Ã— 2 strategies = ~60 strategy evaluations
Each evaluation: ~1.5-2 seconds average

Example timeline:
00:15:13 - BTCUSD MaxDiff evaluation
00:15:13 - BTCUSD MaxDiffAlwaysOn evaluation
00:15:16 - Next symbol...
(pattern repeats)
```

**ğŸŸ¡ Optimization Opportunity**: Strategy evaluations are sequential

### Performance Breakdown (Wall Clock Time)

```
Module Imports:             0.025s  ( 0.02%)
Alpaca API:                 1s      ( 0.67%)
Model Loading:              47s     (31.33%)  ğŸ”´ BOTTLENECK
Data Fetching:              5s      ( 3.33%)
MaxDiff Evaluations:        95s     (63.33%)  ğŸŸ¡ OPTIMIZATION TARGET
Other:                      2s      ( 1.33%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                      150s    (100.00%)
```

## Bottleneck Analysis

### ğŸ”´ Critical: torch.compile First-Run Penalty

**Impact**: 35-40 seconds (26% of total runtime)

**Why It Happens**:
- torch.compile uses ahead-of-time compilation
- First run generates optimized CUDA kernels
- Kernels are cached in `compiled_models/torch_inductor/`
- Subsequent runs are fast (kernel cache hit)

**Current Behavior** (from code):
```python
backtest_test3_inline.py:2073
Using torch.compile for Toto (mode=reduce-overhead, backend=inductor)
```

**Optimization Strategies**:

1. **Kernel Precompilation** (Best ROI)
   ```python
   # Warm up the model with dummy data on startup
   # This pre-compiles kernels before real trading
   with torch.no_grad():
       dummy_context = torch.randn(1, context_length, device='cuda')
       model(dummy_context)  # Triggers compilation
   ```

2. **Persistent Kernel Cache**
   - Current cache: `compiled_models/torch_inductor/`
   - Ensure cache persists across runs
   - Cache is machine + GPU specific

3. **Consider Eager Mode for First Analysis**
   ```python
   # Use torch.compile only after first cycle completes
   # Trade first-run latency for startup speed
   ```

### ğŸŸ¡ Major: Sequential Strategy Evaluation

**Impact**: 95 seconds (63% of total runtime)

**Current Pattern**:
```python
for symbol in symbols:  # ~30 symbols
    for strategy in [MaxDiff, MaxDiffAlwaysOn]:
        evaluate(symbol, strategy)  # ~1.5s each
```

**Parallelization Opportunities**:

1. **Symbol-Level Parallelization** (Easiest)
   ```python
   from concurrent.futures import ProcessPoolExecutor

   with ProcessPoolExecutor(max_workers=4) as executor:
       results = executor.map(analyze_symbol, symbols)
   ```
   **Expected Speedup**: 3-4x (on 4-core system)

2. **Strategy-Level Parallelization**
   ```python
   # Evaluate both strategies concurrently per symbol
   with ThreadPoolExecutor(max_workers=2) as executor:
       maxdiff_future = executor.submit(evaluate_maxdiff, symbol)
       always_on_future = executor.submit(evaluate_maxdiff_always_on, symbol)
   ```
   **Expected Speedup**: 1.8-2x

3. **Batch Processing**
   ```python
   # If model supports batching, process multiple symbols at once
   batch_size = 4
   for batch in chunks(symbols, batch_size):
       evaluate_batch(batch)
   ```
   **Expected Speedup**: Depends on GPU utilization

### ğŸŸ¢ Minor: Data Fetching

**Impact**: 5 seconds (3% of total runtime)

Already quite fast. Potential micro-optimizations:
- Concurrent API calls (if Alpaca rate limits allow)
- Local caching for non-real-time data

## Memory Profile (GPU)

```
Phase                           Allocated    Reserved    Peak
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
After Toto load:               609.7 MB     652.2 MB    609.7 MB
After first inference:         1025.8 MB    ???         1025.8 MB
Delta (inference overhead):    416.1 MB     ???         416.1 MB
```

**Note**: Model uses ~610 MB, inference adds ~416 MB (activations, gradients)

## Optimization Priority Matrix

| Optimization | Effort | Impact | ROI | Priority |
|-------------|--------|--------|-----|----------|
| Warm-up torch.compile | Low | High (40s saved) | â­â­â­â­â­ | **P0** |
| Symbol parallelization | Medium | High (60-70s saved) | â­â­â­â­ | **P1** |
| Strategy parallelization | Low | Medium (30-40s saved) | â­â­â­â­ | **P1** |
| Kernel cache management | Low | High (first run only) | â­â­â­ | P2 |
| Batch inference | High | Medium-High (depends) | â­â­ | P3 |
| Data fetch parallelization | Medium | Low (2-3s saved) | â­ | P4 |

## Expected Results After Optimization

### Scenario 1: Quick Wins (P0 + P1)

```
Current:        150s baseline
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-40s torch.compile warmup (happens once, then cached)
-60s symbol parallelization (4 workers)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Result:         50s (first run after restart)
                10s (subsequent runs with warm cache)
Improvement:    66-93% faster
```

### Scenario 2: Full Optimization (P0-P3)

```
Current:        150s baseline
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-40s torch.compile warmup
-70s symbol + strategy parallelization
-5s  batch inference optimization
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Result:         35s (first run after restart)
                8s  (subsequent runs)
Improvement:    76-94% faster
```

## CUDA Kernel Compilation Details

From stderr logs:
```
skipping cudagraphs due to mutated inputs (2 instances)
```

**Meaning**: torch.compile detected dynamic input shapes, preventing CUDA graph optimization

**Impact**: Minor - CUDA graphs would save ~5-10% on inference, but require fixed input shapes

**Fix** (if applicable):
```python
# Ensure consistent tensor shapes for CUDA graph support
# Pad sequences to fixed length if needed
```

## Recommendations

### Immediate Actions (< 1 hour implementation)

1. **Add model warmup on startup**
   ```python
   # In backtest_test3_inline.py after model load
   def warmup_model(pipeline, context_length):
       with torch.no_grad():
           dummy = torch.randn(1, context_length, device='cuda')
           pipeline(dummy)
       logger.info("Model warmup complete - kernels compiled")
   ```

2. **Profile one full analysis cycle**
   ```bash
   # Let profiler run for 10-15 minutes to capture full cycle
   python profile_trade_stock.py
   # Wait for "INITIAL ANALYSIS COMPLETE" or similar
   # Then Ctrl+C
   ```

### Short-term (< 1 week)

3. **Implement symbol-level parallelization**
   - Use ProcessPoolExecutor for symbol analysis
   - Start with 2-4 workers based on CPU cores
   - Monitor GPU memory (might need sequential model loading)

4. **Add performance metrics**
   ```python
   # Track timing for each phase
   @contextmanager
   def timer(name):
       start = time.time()
       yield
       logger.info(f"{name}: {time.time()-start:.2f}s")

   with timer("Model inference"):
       predictions = model(data)
   ```

### Long-term (> 1 week)

5. **Investigate batch inference** for strategy evaluation
6. **Profile memory usage** to optimize GPU utilization
7. **Consider mixed precision** (FP16) for inference speedup

## Next Steps

1. âœ… Install flamegraph tooling
2. âœ… Profile startup phase
3. ğŸ”² Profile full analysis cycle (10-15 min run)
4. ğŸ”² Implement model warmup (P0)
5. ğŸ”² Benchmark before/after warmup
6. ğŸ”² Implement parallelization (P1)
7. ğŸ”² Benchmark parallel vs sequential

## Profiling Commands Reference

```bash
# Profile startup only (2-3 min)
python profile_trade_stock.py
# Ctrl+C after model loading completes

# Profile full cycle (10-15 min)
python profile_trade_stock.py
# Ctrl+C after "INITIAL ANALYSIS COMPLETE"

# Generate flamegraph
.venv/bin/python -m flameprof trade_stock_e2e_paper.prof -o flamegraph.svg

# Analyze flamegraph
.venv/bin/flamegraph-analyzer flamegraph.svg -o analysis.md

# View interactively
xdg-open flamegraph.svg
```

## Conclusion

The PAPER mode analysis reveals:

1. **Startup is fast**: 25ms for all Python imports âœ…
2. **torch.compile is the startup bottleneck**: 40s first-run penalty ğŸ”´
3. **Strategy evaluation dominates runtime**: 63% of total time ğŸŸ¡
4. **Significant optimization potential**: 66-93% speedup achievable

**Recommended focus**: P0 (warmup) + P1 (parallelization) = biggest bang for buck
